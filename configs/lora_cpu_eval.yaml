# Configuration for CPU-only LoRA adapter evaluation
# Used by: src/evaluation/lora_cpu_eval.py
# Note: This config is merged with base.yaml in the Python code

# CPU-only LoRA evaluation configuration
lora_cpu_eval:
  # Base model to load
  base_model: Qwen/Qwen3-0.6B

  # Adapter configuration
  adapter:
    source: hub  # Options: 'local', 'hub', or 'none' (for base model only)
    # For hub: use Hugging Face Hub repo ID (e.g., 'username/model-name-lora-dataset')
    path: saurluca/Qwen3-0.6B-lora-gras
    # For local: path to adapter directory (relative to project root)
    # path: ${paths.output_dir}/peft_output/adapter-Qwen3-0.6B-gras
    # For none: no adapter will be loaded, base model will be used directly

  # Dataset configuration
  dataset:
    csv_path: data/gras/test.csv
    
    # Data sampling for evaluation (optional)
    sample_fraction: 1.0  # Use 100% of data by default (0.0-1.0)
    sample_seed: 42  # Seed for reproducible sampling

  # Evaluation settings
  batch_size: 8  # CPU-friendly batch size (smaller than GPU)
  enforce_cpu: true  # Enforce CPU-only execution
  
  # Output directory for evaluation results
  output_dir: ${paths.output_dir}/lora_cpu_eval

# Tokenization settings (should match training config)
tokenization:
  include_reference_answer: true  # Match training config for gras

# Output settings
output:
  dir: ${paths.results_dir}
  save_predictions: false
  save_confusion_matrix: false
  save_classification_report: false

