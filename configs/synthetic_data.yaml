# models: apertus-8b, apertus-70b, gpt-4o-mini, gpt-4o, llama3.2:3b, deepseek-r1, llama3.1:8b
model_name: gpt-4o
teacher_model_name: gpt-4o-mini

# Raw input folders (JSON)
raw_tasks_dir: data/raw/tasks
raw_chunks_dir: data/raw/chunks

# Synth output (CSV)
output_dir: data/synth
# tasks_filename: tasks_unified.csv
tasks_filename: all_tasks.csv

num_correct_answers: 3
num_partial_answers: 3
num_incorrect_answers: 3

lm_temp_eval: 0.6
lm_temp_generation: 1.0
lm_cache: true

# Evaluation batching mode: single | per_question | all
eval_mode: per_question
create_mode: per_question

# Control which fields are passed to models
# Evaluation flags
chain_of_thought: false
eval_pass_reference: false # pass chunk_text as `reference`
eval_pass_reference_answer: true # pass ground-truth as `reference_answer`

# Creation flags (per generator type)
generation:
  chain_of_thought: false

create_pass_reference_for_correct: false
create_pass_reference_answer_for_correct: true
create_pass_reference_for_partial: false
create_pass_reference_answer_for_partial: true
create_pass_reference_for_incorrect: false
create_pass_reference_answer_for_incorrect: false
# total possible combinations:
#20 (questions) * 3 (answers) * 3 (generation) * 3 (eval) * 4 (possible infor gen) * 4 (eval info) = 20 * 3 * 3 * 3 * 4 * 4 = 8640
