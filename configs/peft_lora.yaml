# Minimal PEFT/LoRA config for classifying student answers

# Small base model that works well with LoRA and runs on CPU/GPU easily
# model_name: distilbert-base-uncased
model_name: meta-llama/Llama-3.2-1B-Instruct

# Path to the CSV produced by synthetic_data/create_synth_dataset.py
# Expected columns: question, student_answer, intended_label (incorrect|partial|correct)
dataset_csv: data/synth/student_answers_c6_p6_i6_gpt-4o-mini_per_question.csv

# Where to save model outputs and adapter
output_dir: data/peft_output
hf_cache_dir: .hf_cache

seed: 42

lora:
  r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  # For DistilBERT, target q_lin/v_lin in attention blocks
  # target_modules: ["q_lin", "v_lin"]
  target_modules: ["q_proj", "v_proj"]

training:
  num_train_epochs: 5
  # per_device_train_batch_size: 32
  # per_device_eval_batch_size: 32
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  learning_rate: 0.0001
  weight_decay: 0.0001
  eval_strategy: epoch
  save_strategy: epoch
  logging_steps: 10
