# Minimal PEFT/LoRA config for classifying student answers

# Small base model that works well with LoRA and runs on CPU/GPU easily
# model_name: distilbert-base-uncased
# model_name: meta-llama/Llama-3.2-3B-Instruct
model_name: meta-llama/Llama-3.2-1B-Instruct
# model_name: swiss-ai/Apertus-8B-Instruct-2509

# Path to the CSV produced by synthetic_data/create_synth_dataset.py
# Expected columns: question, student_answer, intended_label (incorrect|partial|correct)
dataset_csv: data/synth/student_answers_c3_p3_i3_gpt-4o_per_question.csv
# dataset_csv: data/synth/student_answers_c6_p6_i6_gpt-4o-mini_per_question.csv

# Where to save model outputs and adapter
output_dir: data/peft_output
hf_cache_dir: .hf_cache

seed: 42

lora:
  r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  # For DistilBERT, target q_lin/v_lin in attention blocks
  # target_modules: ["q_lin", "v_lin"], q_proj", "v_proj
  target_modules: ["all-linear"]
  init_lora_weights: "olora"

training:
  num_train_epochs: 1
  # per_device_train_batch_size: 32
  # per_device_eval_batch_size: 32
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  learning_rate: 0.0001
  weight_decay: 0.0001
  eval_strategy: epoch
  save_strategy: epoch
  logging_steps: 10
  test_size: 0.5
  use_unseen_questions: true
