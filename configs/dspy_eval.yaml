model:
  # Single model to evaluate
  model: gpt-4o
  # mode: single | per_question
  mode: single
  cache: false
  max_tokens: 4096
  pass_reference_answer: true

dataset:
  # test_csv: data/gras/test.csv
  test_csv: data/SciEntsBank_3way/test_ud.csv

evaluation:
  num_runs: 1 # Number of times to run the evaluation (useful for stochastic models)

mlflow:
  experiment_name: lora_training_comparison_results_final_1 # MLflow experiment name

output:
  dir: ${paths.output_dir}/dspy_eval
