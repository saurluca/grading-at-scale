# Configuration for local classifier evaluation
# Used by: src/evaluation/local.py
# Note: This config is merged with base.yaml in the Python code

# Local classifier evaluation (fine-tuned model evaluation)
classifier_eval:
  # List of models to evaluate
  models:
    - meta-llama/Llama-3.2-1B-Instruct
    - openai-community/gpt2-large
    - google/flan-t5-base
    - Qwen/Qwen3-0.6B

  # Adapter configuration (applies to all models)
  adapter:
    source: local # Options: 'local', 'hub', or 'none' (for base model only)
    # HuggingFace username for hub adapters (required if source is 'hub')
    huggingface_username: saurluca
    # Dataset name for adapter inference (optional, will be inferred from csv_path if not provided)
    dataset_name: gras
    # For 'hub': adapter path will be inferred as {huggingface_username}/{sanitized_model_name}-lora-{dataset_name}
    # For 'local': adapter path will be inferred as ${paths.output_dir}/peft_output/adapter-{sanitized_model_name}-{dataset_name}
    # For 'none': no adapter will be loaded, base model will be used directly

  dataset:
    csv_path: data/SciEntsBank_3way/test_ud.csv
    # csv_path: data/gras/test.csv

    # Data sampling for evaluation
    sample_fraction: 1.0 # Use 100% of data by default (0.0-1.0)
    sample_seed: 24 # Seed for reproducible sampling

  batch_size: 256
  
  # CPU enforcement and evaluation options
  enforce_cpu: false  # Set to true to force CPU-only execution (useful for CPU-only environments)
  timing: false  # Set to true to enable timing metrics (examples/min, time per example)
  report_to: mlflow  # MLflow reporting: "mlflow" or "none" (set to "none" for CPU-only evaluation if MLflow is unavailable)

# Tokenization settings
tokenization:
  include_reference_answer: true

# Output settings
output:
  dir: ${paths.results_dir}
