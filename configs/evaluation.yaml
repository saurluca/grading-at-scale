# Configuration for local classifier evaluation
# Used by: src/evaluation/local.py
# Note: This config is merged with base.yaml in the Python code

# Local classifier evaluation (fine-tuned model evaluation)
classifier_eval:
  # base_model: meta-llama/Llama-3.2-3B-Instruct
  # base_model: google/flan-t5-base
  base_model: Qwen/Qwen3-0.6B

  # Adapter configuration
  adapter:
    source: hub # Options: 'local', 'hub', or 'none' (for base model only)
    # For local: path to adapter directory
    # path: ${paths.output_dir}/peft_output/adapter-flan-t5-base-ASAG2024
    # path: ${paths.output_dir}/peft_output/adapter-Qwen3-0.6B-gras
    # For hub: use Hugging Face Hub repo ID (e.g., 'username/model-name-lora-dataset')
    path: saurluca/Qwen3-0.6B-lora-gras
    # path: saurluca/Qwen3-0.6B-lora-SciEntsBank_3way
    # For none: no adapter will be loaded, base model will be used directly

  dataset:
    load_from_csv: true
    # csv_path: data/synth/dataset_baseline_3-3-3_gpt-4o.csv
    # csv_path: data/synth/full_gas.csv
    # csv_path: data/SciEntsBank_3way/test_ud.csv
    # csv_path: data/ASAG2024/val.csv
    csv_path: data/gras/test.csv
    # else load from dataset directory
    # dir: data/SciEntsBank_3way
    # split: test_ua

    # Data sampling for evaluation
    sample_fraction: 1.0 # Use 100% of data by default (0.0-1.0)
    sample_seed: 24 # Seed for reproducible sampling

  batch_size: 64
  output_dir: ${paths.output_dir}/gras_eval
  
  # CPU enforcement and evaluation options
  enforce_cpu: false  # Set to true to force CPU-only execution (useful for CPU-only environments)
  timing: false  # Set to true to enable timing metrics (examples/min, time per example)
  report_to: mlflow  # MLflow reporting: "mlflow" or "none" (set to "none" for CPU-only evaluation if MLflow is unavailable)

# Tokenization settings
tokenization:
  include_reference_answer: true

# Output settings
output:
  dir: ${paths.results_dir}
  save_predictions: false
  save_confusion_matrix: true
  save_classification_report: false
