# Configuration for model evaluation
# Used by: src/evaluation/evaluate_*.py
# Note: This config is merged with base.yaml in the Python code

# API-based evaluation (DSPy graders)
api_eval:
  model: gpt-4o-mini
  temperature: 0.6
  cache: true
  max_tokens: 512

  mode: per_question # single | per_question | all
  chain_of_thought: false

  # What to pass to grader
  pass_reference: false
  pass_reference_answer: true

  # Data source
  data:
    infer_path: false # Auto-infer from generation config
    # manual_path: data/synth/dataset_baseline_3-3-3_gpt-4o.csv # our dataset
    manual_path: data/SciEntsBank_3way/test_u.csv # scientsbank dataset

    # If infer_path=true, uses generation config to build filename
    # Format: student_answers_c{num_correct}_p{num_partial}_i{num_incorrect}_{model}_{mode}.csv
    num_correct_answers: 1
    num_partial_answers: 1
    num_incorrect_answers: 1
    generation_model: gpt-4o-mini
    generation_mode: per_question

# Local classifier evaluation (fine-tuned model on SciEntsBank)
classifier_eval:
  base_model: meta-llama/Llama-3.2-3B-Instruct

  # Adapter configuration
  adapter:
    source: local # Options: 'local' or 'hub'
    # For local: path to adapter directory
    path: ${paths.output_dir}/peft_output/adapter-Llama-3.2-3B-Instruct-SciEntsBank
    # For hub: use Hugging Face Hub repo ID (e.g., 'username/model-name-lora-dataset')
    # path: saurluca/Llama-3.2-1B-Instruct-lora-grading-at-scale

  dataset:
    load_from_csv: true
    # csv_path: data/synth/dataset_baseline_3-3-3_gpt-4o.csv
    csv_path: data/SciEntsBank_3way/test_ud.csv
    # else load from dataset directory
    # dir: data/SciEntsBank_3way
    # split: test_ua

  batch_size: 32
  output_dir: ${paths.output_dir}/scientsbank_eval

# Tokenization settings
tokenization:
  include_reference_answer: true
  include_chunk_text: false

# Output settings
output:
  dir: ${paths.results_dir}
  save_predictions: false
  save_confusion_matrix: false
  save_classification_report: false
