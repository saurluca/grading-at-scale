# Configuration for model evaluation
# Used by: src/evaluation/evaluate_*.py
# Note: This config is merged with base.yaml in the Python code

# API-based evaluation (DSPy graders)
api_eval:
  # model: llama3.2:1b
  model: gpt-4o-mini
  temperature: 0.6
  cache: true
  max_tokens: 512

  mode: single # single | per_question | all
  chain_of_thought: false

  # What to pass to grader
  pass_reference: false
  pass_reference_answer: false

  # Data source
  data:
    infer_path: false # Auto-infer from generation config
    # manual_path: data/synth/dataset_baseline_3-3-3_gpt-4o.csv # our dataset
    manual_path: data/gras/test.csv

    # If infer_path=true, uses generation config to build filename
    # Format: student_answers_c{num_correct}_p{num_partial}_i{num_incorrect}_{model}_{mode}.csv
    num_correct_answers: 1
    num_partial_answers: 1
    num_incorrect_answers: 1
    generation_model: gpt-4o-mini
    generation_mode: per_question

# Local classifier evaluation (fine-tuned model on SciEntsBank)
classifier_eval:
  # base_model: meta-llama/Llama-3.2-3B-Instruct
  # base_model: google/flan-t5-base
  base_model: Qwen/Qwen3-0.6B

  # Adapter configuration
  adapter:
    source: hub # Options: 'local', 'hub', or 'none' (for base model only)
    # For local: path to adapter directory
    # path: ${paths.output_dir}/peft_output/adapter-flan-t5-base-ASAG2024
    # path: ${paths.output_dir}/peft_output/adapter-Qwen3-0.6B-gras
    # For hub: use Hugging Face Hub repo ID (e.g., 'username/model-name-lora-dataset')
    path: saurluca/Qwen3-0.6B-lora-gras
    # For none: no adapter will be loaded, base model will be used directly

  dataset:
    load_from_csv: true
    # csv_path: data/synth/dataset_baseline_3-3-3_gpt-4o.csv
    # csv_path: data/synth/full_gas.csv
    # csv_path: data/SciEntsBank_3way/test_ua.csv
    # csv_path: data/ASAG2024/val.csv
    csv_path: data/gras/test.csv
    # else load from dataset directory
    # dir: data/SciEntsBank_3way
    # split: test_ua

    # Data sampling for evaluation
    sample_fraction: 1.0 # Use 100% of data by default (0.0-1.0)
    sample_seed: 24 # Seed for reproducible sampling

  batch_size: 1
  output_dir: ${paths.output_dir}/gras_eval

# Tokenization settings
tokenization:
  include_reference_answer: false
  include_chunk_text: false

# Output settings
output:
  dir: ${paths.results_dir}
  save_predictions: false
  save_confusion_matrix: false
  save_classification_report: false
