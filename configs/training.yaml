# Configuration for model training/fine-tuning
# Used by: src/optimisation/finetune_*.py
# Note: This config is merged with base.yaml in the Python code

# Model selection
model:
  base: meta-llama/Llama-3.2-1B-Instruct
  type: lora # vanilla | lora | lora_quantized | lora_typst

# Dataset
dataset:
  csv_path: data/synth/dataset_baseline_3-3-3_gpt-4o.csv
  # csv_path: data/synth/dataset_short.csv
  # csv_path: data/SciEntsBank_3way/train.csv
  test_size: 0.5
  use_unseen_questions: true # Split by question ID, not random
  # dataset_name: SciEntsBank
  dataset_name: grading-at-scale

# Output
output:
  dir: ${paths.output_dir}/peft_output
  save_model: true
  save_strategy: epoch

# LoRA configuration (used for lora, lora_quantized, lora_typst types)
lora:
  r: 16
  alpha: 16
  dropout: 0.05
  target_modules: [all-linear] # or ["q_proj", "v_proj"]
  init_weights: olora

# Quantization (only used if model.type == lora_quantized)
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: float16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true

# Training hyperparameters
training:
  num_epochs: 2
  batch_size:
    train: 4
    eval: 6
  learning_rate: 0.0001
  weight_decay: 0.001
  eval_strategy: epoch
  logging_steps: 10

# Tokenization settings
tokenization:
  include_reference_answer: true
  include_chunk_text: false

# MLflow tracking
mlflow:
  experiment_name: peft_lora_training
  tracking_uri: null # Uses default (./mlruns)

# Typst-specific settings (only used if model.type == lora_typst)
typst:
  dataset: TechxGenus/Typst-Train
  typst_only: true
  test_size: 0.1
  block_size: 256
  num_proc: 8
  quick_run: false

  # Hugging Face Hub settings
  hub:
    push_to_hub: true
    repo_id: null
    private: false
    token_env: HUGGING_FACE_HUB_TOKEN
    commit_message: "Upload LoRA adapter"
