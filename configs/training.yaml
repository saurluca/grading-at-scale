# Model selection
model:
  # base: meta-llama/Llama-3.2-1B-Instruct
  # base: openai-community/gpt2-large
  # base: meta-llama/Llama-3.2-3B-Instruct
  base: openai-community/gpt2
  # base: google/flan-t5-base
  # base: Qwen/Qwen3-0.6B
  # base: swiss-ai/Apertus-8B-Instruct-2509

# Task selection for dispatcher
task_type: lora-classification  # lora-classification | lora-regression | vanilla-classification

# Dispatcher controls
dispatcher:
  num_runs: 3
  seed_strategy: random  # same | random
  seeds: [42,43,44]          # optional list; if provided, overrides both above
  # models: [openai-community/gpt2, openai-community/gpt2-large, google/flan-t5-base, Qwen/Qwen3-0.6B, meta-llama/Llama-3.2-1B-Instruct]              # optional list of model base names to run; if empty uses model.base

# Dataset
dataset:
  # dataset_name: ASAG2024
  dataset_name: gras
  csv_file: full.csv
  csv_path: data/${dataset.dataset_name}/${dataset.csv_file}
  test_size: 0.4
  use_unseen_questions: true # Split by question ID, not random
  # topics: ["SciEntsBank", "Beetle", "Mohler", "DigiKlausur", "SAF", "Stita", "CU-NLP"]
  # topics: ["language", "ai", "neuro", "memory"] 
  topics: [] # null or empty list = all topics

# Output
output:
  dir: ${paths.output_dir}/peft_output
  save_model_locally: true
  push_to_hub: false
  save_strategy: epoch

# LoRA configuration (used for lora, lora_quantized, lora_typst types)
lora:
  r: 16
  alpha: 16
  dropout: 0.1
  target_modules: "all-linear" # or ["q_proj", "v_proj"]
  # target_modules: ["c_attn", "c_proj", "c_fc"]  # Include feed-forward layers too
  init_weights: False # False disables init_lora_weights
  use_rslora: False # False disables use_rslora

# Quantization settings for LoRA training
# Set load_in_4bit to true to enable 4-bit quantization (QLoRA)
# Set to false for full precision training
quantization:
  load_in_4bit: false  # Set to true to enable 4-bit quantization
  bnb_4bit_use_double_quant: false  # Use double quantization for better accuracy
  bnb_4bit_compute_dtype: float16  # Options: float16, bfloat16
  bnb_4bit_quant_type: nf4  # Options: nf4, fp4


# Training hyperparameters
training:
  num_epochs: 2 # 3
  batch_size: 
    train: 8
    eval: 16
  gradient_accumulation_steps: 1  # batch_size * gradient_accumulation_steps = 16
  learning_rate: 0.0002 # 0.0002
  weight_decay: 0.01 # 0.01
  eval_strategy: epoch # epoch | steps
  logging_steps: 20 # 20

# Tokenization settings
tokenization:
  include_reference_answer: false # true
  include_chunk_text: false # false

# MLflow tracking
mlflow:
  experiment_name: lora_training_testing


# torch_empty_cache_steps to reduce vram