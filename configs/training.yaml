# Model selection
model:
  # base: meta-llama/Llama-3.2-1B-Instruct
  # base: openai-community/gpt2-large
  # base: meta-llama/Llama-3.2-3B-Instruct
  # base: openai-community/gpt2
  # base: google/flan-t5-base
  base: Qwen/Qwen3-0.6B

# Task selection for dispatcher
task_type: lora-classification  # lora-classification | lora-regression | vanilla-classification

# Dispatcher controls
dispatcher:
  num_runs: 1
  seed_strategy: random  # same | random
  # seeds: [42,43,44]          # optional list; if provided, overrides both above
  # optional list of model base names to run; if empty uses model.base
  # models: [
  #   meta-llama/Llama-3.2-1B-Instruct,
  #   openai-community/gpt2-large,
  #   openai-community/gpt2,
  #   google/flan-t5-base,
  #   Qwen/Qwen3-0.6B,
  # ]             


# Dataset
dataset:
  # dataset_name: ASAG2024
  dataset_name: gras
  
  # Option 1: Separate train/val/test files - set use_split_files: true
  use_split_files: true  # If true, uses train.csv and val.csv from dataset folder
  train_file: train.csv
  val_file: val.csv
  test_file: test.csv  # Currently ignored, val.csv used for testing'

  # Option 2: Single CSV file - will be split at runtime
  csv_file: train.csv  # Single file to split at runtime
  csv_path: data/${dataset.dataset_name}/${dataset.csv_file}
  
  
  test_size: 0.4  # Only used when use_split_files: false
  use_unseen_questions: true # Split by question ID, not random (only for runtime split)
  # topics: ["SciEntsBank", "Beetle", "Mohler", "DigiKlausur", "SAF", "Stita", "CU-NLP"]
  # topics: ["language", "ai", "neuro", "memory"] 
  topics: [] # null or empty list = all topics

# Output
output:
  dir: ${paths.output_dir}/peft_output
  save_model_locally: false
  push_to_hub: false
  save_strategy: epoch

# LoRA configuration
lora:
  r: 16
  alpha: 16
  dropout: 0.1
  target_modules: "all-linear"

# Training hyperparameters
training:
  num_epochs: 3 # 3
  batch_size: 
    train: 8 # 8
    eval: 16 # 16
  gradient_accumulation_steps: 1  # batch_size * gradient_accumulation_steps = 8
  learning_rate: 0.0002 # 0.0002
  weight_decay: 0.01 # 0.01
  eval_strategy: epoch # epoch | steps
  logging_steps: 20 # 20

# Tokenization settings
tokenization:
  include_reference_answer: true # true
  include_chunk_text: false # false

# MLflow tracking
mlflow:
  experiment_name: lora_training_comparison_v2
