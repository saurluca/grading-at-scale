# Configuration for model training/fine-tuning
# Used by: src/optimisation/finetune_*.py
# Note: This config is merged with base.yaml in the Python code

# Model selection
model:
  base: meta-llama/Llama-3.2-1B-Instruct
  # base: swiss-ai/Apertus-8B-Instruct-2509

# Dataset
dataset:
  # dataset_name: ASAG2024
  dataset_name: gas
  csv_file: smoll.csv
  csv_path: data/${dataset.dataset_name}/${dataset.csv_file}
  # csv_path: data/synth/dataset_baseline_3-3-3_gpt-4o.csv
  # csv_path: data/synth/full_gas.csv
  # csv_path: data/SciEntsBank_3way/train.csv
  test_size: 0.5
  use_unseen_questions: false # Split by question ID, not random
  # topics: ["SciEntsBank", "Beetle", "Mohler", "DigiKlausur", "SAF", "Stita", "CU-NLP"]
  # dataset_name: grading-at-scale
  # topics: ["language", "ai", "neuro", "memory"] # null or empty list = all topics
  topics: null

# Output
output:
  dir: ${paths.output_dir}/peft_output
  save_model: false
  save_strategy: epoch

# LoRA configuration (used for lora, lora_quantized, lora_typst types)
lora:
  r: 16
  alpha: 16
  dropout: 0.1
  target_modules: "all-linear" # or ["q_proj", "v_proj"]
  init_weights: olora

# Quantization settings for LoRA training
# Set load_in_4bit to true to enable 4-bit quantization (QLoRA)
# Set to false for full precision training
quantization:
  load_in_4bit: false  # Set to true to enable 4-bit quantization
  bnb_4bit_use_double_quant: false  # Use double quantization for better accuracy
  bnb_4bit_compute_dtype: float16  # Options: float16, bfloat16
  bnb_4bit_quant_type: nf4  # Options: nf4, fp4


# Training hyperparameters
training:
  num_epochs: 1 # 3
  batch_size: 
    train: 4
    eval: 4
  gradient_accumulation_steps: 1  # batch_size * gradient_accumulation_steps = 16
  learning_rate: 0.0002 # 0.0002
  weight_decay: 0.01 # 0.01
  eval_strategy: epoch # epoch | steps
  logging_steps: 20 # 20

# Tokenization settings
tokenization:
  include_reference_answer: false # true
  include_chunk_text: false # false

# MLflow tracking
mlflow:
  experiment_name: peft_lora_training