# Dispatcher controls
dispatcher:
  num_runs: 1
  # seeds: [42,43,44]  # optional list; if not provided, random seeds will be used
  models: 
    - meta-llama/Llama-3.2-1B-Instruct
    - openai-community/gpt2-large
    - google/flan-t5-base
    - Qwen/Qwen3-0.6B

# Dataset
dataset:
  # dataset_name: SciEntsBank_3way # gras
  dataset_name: gras
  
  train_file: train.csv
  val_file: val.csv
  test_file: test.csv

# Output
output:
  dir: ${paths.output_dir}/peft_output
  save_model_locally: true
  push_to_hub: false
  save_strategy: steps

# LoRA configuration
lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: "all-linear"

# Training hyperparameters
training:
  num_epochs: 5 # 3
  batch_size: 
    train: 16 # 16
    eval: 128 # 16
  gradient_accumulation_steps: 1  # batch_size * gradient_accumulation_steps = 8
  learning_rate: 0.0005 # 0.0002
  weight_decay: 0.01 # 0.01
  eval_strategy: steps # epoch | steps
  eval_steps: 100  # Evaluation frequency in steps (only used when eval_strategy is overridden to steps)
  early_stopping_patience: 3  # Number of evaluations to wait before early stopping
  logging_steps: 20 # 20

# Tokenization settings
tokenization:
  include_reference_answer: true # true

# MLflow tracking
mlflow:
  experiment_name: lora_training_comparison_v3

# Topic k-fold cross-validation settings
kfold:
  train_topics: 3  # Number of topics to use for training (remaining topics used for testing)
                   # Options: 2 (train on 2, test on 2) or 3 (train on 3, test on 1)
                   # Default: 3
  out_of_fold_samples: 5  # Number of samples from test topics to include in training set
                          # Set to 0 to disable (default: 0)
                          # Useful for few-shot learning experiments
