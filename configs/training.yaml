# Model selection
model:
  # base: meta-llama/Llama-3.2-1B-Instruct
  # base: openai-community/gpt2-large
  # base: google/flan-t5-base
  base: Qwen/Qwen3-0.6B

# Task selection for dispatcher
task_type: lora-classification  # lora-classification | lora-classification-gridsearch

# Dispatcher controls
dispatcher:
  num_runs: 5
  seed_strategy: random  # same | random
  # seeds: [42,43,44]          # optional list; if provided, overrides both above
  # optional list of model base names to run; if empty uses model.base
  models: 
    - meta-llama/Llama-3.2-1B-Instruct
    - openai-community/gpt2-large
    - google/flan-t5-base
    - Qwen/Qwen3-0.6B


# Dataset
dataset:
  # dataset_name: SciEntsBank_3way # gras
  dataset_name: gras
  
  # Option 1: Separate train/val/test files - set use_split_files: true
  use_split_files: true  # If true, uses train.csv, val.csv, and test.csv from dataset folder
  train_file: train.csv
  val_file: val.csv #test_ua.csv #val.csv
  test_file: test.csv #test_ud.csv #test.csv  # Required when use_split_files: true

  # Option 2: Single CSV file - will be split at runtime
  csv_file: full.csv  # Single file to split at runtime
  csv_path: data/${dataset.dataset_name}/${dataset.csv_file}
  
  
  test_size: 0.4  # Only used when use_split_files: false
  # topics: ["SciEntsBank", "Beetle", "Mohler", "DigiKlausur", "SAF", "Stita", "CU-NLP"]
  # topics: ["language", "ai", "neuro", "memory"] 
  topics: [] # null or empty list = all topics

# Output
output:
  dir: ${paths.output_dir}/peft_output
  save_model_locally: false
  push_to_hub: false
  save_strategy: steps

# LoRA configuration
lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: "all-linear"

# Training hyperparameters
training:
  num_epochs: 4 # 3
  batch_size: 
    train: 16 # 16
    eval: 64 # 16
  gradient_accumulation_steps: 1  # batch_size * gradient_accumulation_steps = 8
  learning_rate: 0.0005 # 0.0002
  weight_decay: 0.01 # 0.01
  eval_strategy: steps # epoch | steps
  eval_steps: 100  # Evaluation frequency in steps (only used when eval_strategy is overridden to steps)
  early_stopping_patience: 3  # Number of evaluations to wait before early stopping
  logging_steps: 20 # 20

# Tokenization settings
tokenization:
  include_reference_answer: true # true

# MLflow tracking
mlflow:
  experiment_name: lora_training_comparison_results

# Topic k-fold cross-validation settings
kfold:
  train_topics: 3  # Number of topics to use for training (remaining topics used for testing)
                   # Options: 2 (train on 2, test on 2) or 3 (train on 3, test on 1)
                   # Default: 3
  out_of_fold_samples: 5  # Number of samples from test topics to include in training set
                          # Set to 0 to disable (default: 0)
                          # Useful for few-shot learning experiments
