# Hyperparameter Grid Search Configuration
# This config extends training.yaml and defines grid search parameters

# Dispatcher controls (for multiple runs with different seeds)
dispatcher:
  num_runs: 1  # Number of grid search runs to perform
  # seeds: [42, 43, 44]  # optional list; if not provided, random seeds will be used
  # If seeds is not specified, random seeds will be generated based on num_runs

# Model selection (single model only for grid search)
model:
  # base: Qwen/Qwen3-0.6B
  # base: meta-llama/Llama-3.2-1B-Instruct
  base: openai-community/gpt2-large
  # base: google/flan-t5-large

# Grid search parameters
grid_search:
  # Optimization metric to use for selecting best combination
  optimization_metric: macro_f1  # weighted_f1 | macro_f1 | accuracy | eval_loss
  
  # Learning rate values to explore
  learning_rate: [0.0001, 0.0002, 0.0005, 0.001]
  
  # LoRA rank (r) values to explore
  lora_r: [8]
  
  # LoRA alpha ratios (alpha will be computed as r * ratio)
  lora_alpha_ratios: [2.0]
  
  # LoRA dropout values to explore
  lora_dropout: [0.2]
  
  batch_size: [8, 16, 32]

# Dataset (inherits from training.yaml if not specified)
dataset:
  dataset_name: gras
  train_file: train.csv
  val_file: val.csv
  test_file: test.csv

# Output
output:
  dir: ${paths.output_dir}/peft_output_gridsearch
  save_model_locally: false
  push_to_hub: false
  save_strategy: epoch

# LoRA configuration (base values, will be overridden by grid_search)
lora:
  r: 8
  alpha: 16
  dropout: 0.2
  target_modules: "all-linear"

# Training hyperparameters (base values, will be overridden by grid_search)
training:
  num_epochs: 4
  batch_size: 
    train: 16
    eval: 128
  gradient_accumulation_steps: 1  # Fixed at 1, batch_size is varied instead
  weight_decay: 0.01
  eval_strategy: steps
  eval_steps: 100
  early_stopping_patience: 3
  logging_steps: 20

# Tokenization settings
tokenization:
  include_reference_answer: true

# MLflow tracking
mlflow:
  experiment_name: lora_gridsearch_v3

