# Vanilla (non-LoRA) fine-tuning config for classifying student answers

# Model name
model_name: meta-llama/Llama-3.2-1B-Instruct
# model_name: swiss-ai/Apertus-8B-Instruct-2509

# Dataset CSV path
# Expected columns: question, student_answer, label (incorrect|partial|correct)
dataset_csv: data/synth/student_answers_c6_p6_i6_gpt-4o-mini_per_question.csv

# Output and cache directories
output_dir: data/vanilla_output
hf_cache_dir: .hf_cache

seed: 42

training:
  num_train_epochs: 2
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  learning_rate: 0.0001
  weight_decay: 0.0001
  eval_strategy: epoch
  save_strategy: epoch
  logging_steps: 10
