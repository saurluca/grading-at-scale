# LoRA config for Typst Causal LM finetuning on TechxGenus/Typst-Train

# Base model suitable for Causal LM with LoRA
model_name: meta-llama/Llama-3.2-1B-Instruct
# model_name: distilbert/distilroberta-base

# Where to save model outputs and adapter
output_dir: data/peft_output
hf_cache_dir: .hf_cache

seed: 42

# Quick dry-run: train a single batch to validate the pipeline
quick_run: true

# Dataset-related knobs
typst_only: true # filter rows where language == "typst"
test_size: 0.1 # fraction of data used for evaluation split
block_size: 256 # 1024
num_proc: 8 # parallel workers for tokenization/grouping (auto if unset)

lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  # target_modules: ["q_proj", "v_proj"]
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

training:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  learning_rate: 0.0003
  weight_decay: 0.01
  eval_strategy: epoch
  save_strategy: epoch
  logging_steps: 10

# Hugging Face Hub settings
hub:
  push_to_hub: true
  # Set to your namespace/repo-name, e.g. "your-username/typst-lora-adapter"
  repo_id: saurluca/llama-3.2-1b-typst-test
  private: false
  # Name of environment variable that holds your HF token
  token_env: HUGGING_FACE_HUB_TOKEN
  commit_message: "Upload LoRA adapter"
