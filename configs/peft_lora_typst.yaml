# LoRA config for Typst Causal LM finetuning on TechxGenus/Typst-Train

# Base model suitable for Causal LM with LoRA
model_name: meta-llama/Llama-3.2-1B-Instruct
# model_name: distilbert/distilroberta-base

# Where to save model outputs and adapter
output_dir: data/peft_output
hf_cache_dir: .hf_cache

seed: 42

# Dataset-related knobs
typst_only: true # filter rows where language == "typst"
test_size: 0.1 # fraction of data used for evaluation split
block_size: 256 # token block size for grouping in causal LM
num_proc: 8 # parallel workers for tokenization/grouping (auto if unset)

lora:
  r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  # For Llama models, q_proj and v_proj are common LoRA targets
  target_modules: ["q_proj", "v_proj"]

training:
  num_train_epochs: 3
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  learning_rate: 0.0001
  weight_decay: 0.0001
  eval_strategy: epoch
  save_strategy: epoch
  logging_steps: 10
